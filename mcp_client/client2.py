# # app.py â€” Minimal MCP + Streamlit chat (correct tool message ordering, no filler rendered)

# import os
# import json
# import asyncio
# import streamlit as st
# from dotenv import load_dotenv

# # from langchain_openai import ChatOpenAI
# from langchain_google_genai import ChatGoogleGenerativeAI

# from langchain_mcp_adapters.client import MultiServerMCPClient
# from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # MCP servers: local math via uv + fastmcp
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SERVERS = { 
#     "Aditya Server Proxy": {
#         "transport": "streamable_http",  # if this fails, try "sse"
#         "url": "https://aggregate-rose-orca.fastmcp.app/mcp"
#     },
#     "manim-server": {
#       "command": "C:/Users/aditya.mishra01/AppData/Local/Programs/Python/Python313/python.exe",
#       "args": [
#         "C:/Users/aditya.mishra01/Downloads/manim-mcp-server/src/manim_server.py"
#       ],
#       "env": {
#         "MANIM_EXECUTABLE": "C:/Users/aditya.mishra01/AppData/Local/Programs/Python/Python313/Scripts/manim.exe"
#       },
#       "transport": "stdio",
#     },
# }


# SYSTEM_PROMPT = (
#     "You have access to tools. When you choose to call a tool, do not narrate status updates. "
#     "After tools run, return only a concise final answer."
# )

# st.set_page_config(page_title="MCP Chat", page_icon="ğŸ§°", layout="centered")
# st.title("ğŸ§° MCP Chat")

# load_dotenv()

# # One-time init
# if "initialized" not in st.session_state:
#     # 1) LLM
#     st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")


#     # 2) MCP tools
#     st.session_state.client = MultiServerMCPClient(SERVERS)
#     tools = asyncio.run(st.session_state.client.get_tools())
#     st.session_state.tools = tools
#     st.session_state.tool_by_name = {t.name: t for t in tools}

#     # 3) Bind tools
#     st.session_state.llm_with_tools = st.session_state.llm.bind_tools(tools)

#     # 4) Conversation state
#     st.session_state.history = [SystemMessage(content=SYSTEM_PROMPT)]
#     st.session_state.initialized = True

# # Render chat history (skip system + tool messages; hide intermediate AI with tool_calls)
# for msg in st.session_state.history:
#     if isinstance(msg, HumanMessage):
#         with st.chat_message("user"):
#             st.markdown(msg.content)
#     elif isinstance(msg, AIMessage):
#         # Skip assistant messages that contain tool_calls (intermediate â€œfetchingâ€¦â€)
#         if getattr(msg, "tool_calls", None):
#             continue
#         with st.chat_message("assistant"):
#             st.markdown(msg.content)
#     # ToolMessage and SystemMessage are not rendered as bubbles

# # Chat input
# user_text = st.chat_input("Type a messageâ€¦")
# if user_text:
#     with st.chat_message("user"):
#         st.markdown(user_text)
#     st.session_state.history.append(HumanMessage(content=user_text))

#     # First pass: let the model decide whether to call tools
#     first = asyncio.run(st.session_state.llm_with_tools.ainvoke(st.session_state.history))
#     tool_calls = getattr(first, "tool_calls", None)

#     if not tool_calls:
#         # No tools â†’ show & store assistant reply
#         with st.chat_message("assistant"):
#             st.markdown(first.content or "")
#         st.session_state.history.append(first)
#     else:
#         # â”€â”€ IMPORTANT ORDER â”€â”€
#         # 1) Append assistant message WITH tool_calls (do NOT render)
#         st.session_state.history.append(first)

#         # 2) Execute requested tools and append ToolMessages (do NOT render)
#         tool_msgs = []
#         for tc in tool_calls:
#             name = tc["name"]
#             args = tc.get("args") or {}
#             if isinstance(args, str):
#                 try:
#                     args = json.loads(args)
#                 except Exception:
#                     pass
#             tool = st.session_state.tool_by_name[name]
#             res = asyncio.run(tool.ainvoke(args))
#             tool_msgs.append(ToolMessage(tool_call_id=tc["id"], content=json.dumps(res)))

#         st.session_state.history.extend(tool_msgs)

#         # 3) Final assistant reply using tool outputs â†’ render & store
#         final = asyncio.run(st.session_state.llm.ainvoke(st.session_state.history))
#         with st.chat_message("assistant"):
#             st.markdown(final.content or "")
#         st.session_state.history.append(AIMessage(content=final.content or ""))




# app.py â€” MCP + Streamlit chat (event-loop safe, production ready)

import os
import json
import asyncio
import streamlit as st
from dotenv import load_dotenv

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_core.messages import (
    HumanMessage,
    AIMessage,
    ToolMessage,
    SystemMessage,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MCP server configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SERVERS = {
    "Aditya Server Proxy": {
        "transport": "streamable_http",  # fallback: "sse"
        "url": "https://aggregate-rose-orca.fastmcp.app/mcp",
    },
    "manim-server": {
        "command": "C:/Users/aditya.mishra01/AppData/Local/Programs/Python/Python313/python.exe",
        "args": [
            "C:/Users/aditya.mishra01/Downloads/manim-mcp-server/src/manim_server.py"
        ],
        "env": {
            "MANIM_EXECUTABLE": (
                "C:/Users/aditya.mishra01/"
                "AppData/Local/Programs/Python/Python313/Scripts/manim.exe"
            )
        },
        "transport": "stdio",
    },
}

SYSTEM_PROMPT = (
    "You have access to tools. When you choose to call a tool, "
    "do not narrate status updates. After tools run, return only "
    "a concise final answer."
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Streamlit setup
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="MCP Chat", page_icon="ğŸ§°", layout="centered")
st.title("ğŸ§° MCP Chat")

load_dotenv()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Async helper (CRITICAL)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_async(coro):
    """Run async code on Streamlit-safe persistent loop."""
    return st.session_state.loop.run_until_complete(coro)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# One-time initialization
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "initialized" not in st.session_state:
    # âœ… Create ONE event loop for entire app lifetime
    st.session_state.loop = asyncio.new_event_loop()
    asyncio.set_event_loop(st.session_state.loop)

    # LLM
    st.session_state.llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash"
    )

    # MCP client (created ONCE)
    st.session_state.client = MultiServerMCPClient(SERVERS)

    # Load tools (NO asyncio.run)
    tools = run_async(st.session_state.client.get_tools())
    st.session_state.tools = tools
    st.session_state.tool_by_name = {t.name: t for t in tools}

    # Bind tools to LLM
    st.session_state.llm_with_tools = (
        st.session_state.llm.bind_tools(tools)
    )

    # Conversation history
    st.session_state.history = [
        SystemMessage(content=SYSTEM_PROMPT)
    ]

    st.session_state.initialized = True


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Render chat history
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for msg in st.session_state.history:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.markdown(msg.content)

    elif isinstance(msg, AIMessage):
        # Skip intermediate tool-call messages
        if getattr(msg, "tool_calls", None):
            continue
        with st.chat_message("assistant"):
            st.markdown(msg.content)

    # ToolMessage & SystemMessage are intentionally hidden


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Chat input
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_text = st.chat_input("Type a messageâ€¦")

if user_text:
    # Show user message
    with st.chat_message("user"):
        st.markdown(user_text)

    st.session_state.history.append(
        HumanMessage(content=user_text)
    )

    # â”€â”€ First LLM pass (tool decision) â”€â”€
    first = run_async(
        st.session_state.llm_with_tools.ainvoke(
            st.session_state.history
        )
    )

    tool_calls = getattr(first, "tool_calls", None)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Case 1: No tools needed
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not tool_calls:
        with st.chat_message("assistant"):
            st.markdown(first.content or "")
        st.session_state.history.append(first)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Case 2: Tool calls present
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    else:
        # 1ï¸âƒ£ Append assistant message WITH tool_calls (do not render)
        st.session_state.history.append(first)

        # 2ï¸âƒ£ Execute tools
        tool_messages = []
        for tc in tool_calls:
            tool_name = tc["name"]
            args = tc.get("args") or {}

            if isinstance(args, str):
                try:
                    args = json.loads(args)
                except Exception:
                    pass

            tool = st.session_state.tool_by_name[tool_name]
            result = run_async(tool.ainvoke(args))

            tool_messages.append(
                ToolMessage(
                    tool_call_id=tc["id"],
                    content=json.dumps(result),
                )
            )

        st.session_state.history.extend(tool_messages)

        # 3ï¸âƒ£ Final assistant response
        final = run_async(
            st.session_state.llm.ainvoke(
                st.session_state.history
            )
        )

        with st.chat_message("assistant"):
            st.markdown(final.content or "")

        st.session_state.history.append(
            AIMessage(content=final.content or "")
        )
